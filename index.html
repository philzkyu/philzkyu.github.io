<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Phil D.</title>
  <link rel="stylesheet" href="main.css" />
</head>
<body class="light-theme">

  <!-- Introduction -->
  <header class="hero">
    <img src="phil_circle.png" alt="Phil D." width="225" height="225" />
    <div>
        <h1>Phil D.</h1>
        <h2>AI Safety Research Engineer</h2>
        <p>
          Creator of <a href="https://tinyurl.com/36t53km8" target="_blank" rel="noopener noreferrer"><strong>Project Z</strong></a>,
          an automated red-teaming and safety evaluation framework for frontier AI models. The framework
          is designed to assess how advanced multimodal and language systems behave under real-world conditions,
          with a focus on surfacing hidden reliability and safety issues.
        </p>
        <p>
          Phase 1 uses vision-language models (VLMs) such as CLIP and LLaVA as
          tractable testbeds, expanding toward frontier multilingual and reasoning systems.
        </p>
        <p>
          My work bridges AI safety research with production engineering—building reliable evaluation
          pipelines and safety-critical systems.
        </p>
        <p>
          Pursuing dual-degree Master of Science in Engineering (MSE) in Robotics and
          Computer and Information Science at the University of Pennsylvania (<a href="https://www.grasp.upenn.edu/" target="_blank" rel="noopener noreferrer">GRASP Lab</a>),
          graduating May 2026.
        </p>
    </div>
    </header>

  <br />

  <!-- Research -->
  <section class="section">
    <h2>Research</h2>

    <div class="block">
      <strong>Creator & Lead Research Engineer — Project Z</strong><br />
      (<a href="https://tinyurl.com/36t53km8" target="_blank" rel="noopener">One-page summary</a>)<br />
      <em>Automated Red-Teaming &amp; Safety Evaluation Framework for Frontier Models</em>
      <p class="muted">
        Developing automated red-teaming infrastructure for systematic safety evaluation of frontier models,
        with production-grade CLIP and LLaVA evaluations complete as Phase 1 testbeds.
      </p>
    </div>

    <div class="block">
      <strong>Research Engineer — xLab (Safe Autonomous Systems Lab)</strong><br />
      <em>University of Pennsylvania</em>
      <p class="muted">
       Designed and built safety-critical perception systems for autonomous forklifts, with a focus on deployment
       robustness and failure evaluation.
      </p>
    </div>
  </section>

  <br />

  <!-- Teaching -->
  <section class="section">
    <h2>Teaching</h2>

    <div class="block">
      <strong>Head Teaching Assistant — CIS 5810 Computer Vision &amp; Computational Photography</strong><br />
      <span class="muted">University of Pennsylvania</span>
    </div>

    <div class="block">
      <strong>Head Teaching Assistant — ENGR 2900 Penn Global Seminar: AI &amp; Robotics</strong><br />
      <span class="muted">University of Pennsylvania</span>
    </div>
  </section>

  <br />

  <!-- Awards -->
  <section class="section">
    <h2>Awards</h2>
    <div class="block">
      <strong>TA Award for Excellence (Student-Nominated)</strong> — CIS 5810 Computer Vision &amp; Computational Photography<br />
      <span class="muted">Summer 2022</span>
    </div>
  </section>

  <br />

  <!-- Selected Projects -->
  <section class="section">
    <h2>Selected Projects &amp; Demos</h2>

    <div class="grid">
      <figure>
        <img src="prompt_editing_benchmark_demo.gif" alt="Editing benchmark demo" width="350" height="225" />
        <figcaption>Prompt-based Editing Safety Benchmark</figcaption>
        <p class="caption muted">
          Developing a benchmark for instruction-guided image editing models (InstructPix2Pix, Stable Diffusion Inpainting) 
          to identify capability limitations and failure modes.
        </p>
      </figure>

      <figure>
        <img src="positioning_perspective_demo.gif" alt="Positioning perspective demo" width="350" height="225" />
        <img src="positioning_map_demo.gif" alt="Positioning map demo" width="350" height="225" />
        <figcaption>Real-time Positioning for Robots</figcaption>
        <p class="caption muted">
          Developed a real-time object positioning system using monocular cameras.
        </p>
      </figure>

      <figure>
        <figcaption>LLM Safety Fine-Tuning (LoRA SFT; DPO-ready)</figcaption>
        <p class="caption muted">
          LoRA-tuned TinyLlama-1.1B on Anthropic HH to improve safety refusals, with deterministic harmful-prompt eval and lightweight guardrails (harm/PII checks, sanitized logs); DPO-ready.
        </p>
      </figure>

      <figure>
        <img src="lane_detection_night_demo.gif" alt="Lane detection night demo" width="350" height="225" />
        <img src="lane_detection_day_demo.gif" alt="Lane detection day demo" width="350" height="225" />
        <figcaption>Lane Detection for Autonomous Vehicles</figcaption>
        <p class="caption muted">
          Trained a transformer-based model to detect and segment lane markers.
        </p>
      </figure>

      <figure>
        <img src="techvest_demo.gif" alt="Techvest demo" width="350" height="225" />
        <figcaption>VitalTech SmartVest</figcaption>
        <p class="caption muted">
          Wearable prototype for biometric monitoring (movement/orientation, temperature, heart rate) during exercise.
        </p>
      </figure>

      <figure>
        <img src="text_to_object_segmentation_demo.gif" alt="Text to object segmentation demo" width="350" height="225" />
        <figcaption>Text-to-Object Segmentation</figcaption>
        <p class="caption muted">
          Pipeline for semantic object detection using category-specific bounding-box prompts.
        </p>
      </figure>

      <figure>
        <img src="automated_prompting_demo.png" alt="Automated prompting demo" width="350" height="225" />
        <figcaption>Automated Prompting for SAM</figcaption>
        <p class="caption muted">
          Framework to improve segmentation quality when bounding-box prompts underperform.
        </p>
      </figure>

      <figure>
        <img src="driver_risk_recognition_demo.gif" alt="Driver risk recognition demo" width="350" height="225" />
        <figcaption>Driver Risk Recognition</figcaption>
        <p class="caption muted">
          Application for detecting driver unfitness (e.g., drowsiness or poor road scanning).
        </p>
      </figure>
    </div>
  </section>

  <br />

  <!-- Misc -->
  <section class="section">
    <h2>Misc</h2>
    <p class="muted">
      I enjoy electronic/metal/punk rock, movies, watching baseball, and exploring neighborhoods (and food).
    </p>
    <figure class="block">
      <img src="futuristic_city.jpeg" alt="Futuristic city" width="375" height="300" />
    </figure>
  </section>

  <footer class="footer">
    <span>Contact: <a href="mailto:your.philzkyu@gmail.com">philzkyu@gmail.com</a></span>
    <span>·</span>
  </footer>

  <!-- Footer / Contact -->
<!--
  <footer class="footer">
    <span>Contact: <a href="mailto:yourname@gmail.com">yourname@gmail.com</a></span>
    <span>·</span>
    <a href="resume.pdf" target="_blank" rel="noopener">Resume (PDF)</a>
    <span>·</span>
    <a href="https://github.com/philprojxs" target="_blank" rel="noopener">GitHub</a>
    <span>·</span>
    <a href="https://www.linkedin.com/" target="_blank" rel="noopener">LinkedIn</a>
  </footer>
-->
</body>
</html>
